{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes by - Kiran A Bendigeri\n",
    "Please Read 'Read me' file.\n",
    "\n",
    "1 Hello World\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\win7\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Create a Constant op\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "# Start tf session\n",
    "sess = tf.Session()\n",
    "# Run graph\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 2 b: 3\n",
      "Addition with constants: 5\n",
      "Multiplication with constants: 6\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "\n",
    "# Launch the default graph.\n",
    "with tf.Session() as sess:\n",
    "    print(\"a: %i\" % sess.run(a), \"b: %i\" % sess.run(b))\n",
    "    print(\"Addition with constants: %i\" % sess.run(a+b))\n",
    "    print(\"Multiplication with constants: %i\" % sess.run(a*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.]]\n"
     ]
    }
   ],
   "source": [
    "matrix1 = tf.constant([[3., 3.]])\n",
    "# Create another Constant that produces a 2x1 matrix.\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(product)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "rng = numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1000\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(rng.randn(), name=\"weight\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a linear model\n",
    "pred = tf.add(tf.multiply(X, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared error\n",
    "cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n",
    "# Gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 0.082470603 W= 0.29116723 b= 0.5023885\n",
      "Epoch: 0100 cost= 0.081834726 W= 0.28869477 b= 0.5201753\n",
      "Epoch: 0150 cost= 0.081272401 W= 0.2863692 b= 0.536905\n",
      "Epoch: 0200 cost= 0.080775172 W= 0.28418198 b= 0.5526401\n",
      "Epoch: 0250 cost= 0.080335490 W= 0.28212476 b= 0.56743956\n",
      "Epoch: 0300 cost= 0.079946727 W= 0.2801898 b= 0.58135945\n",
      "Epoch: 0350 cost= 0.079602994 W= 0.27836996 b= 0.59445125\n",
      "Epoch: 0400 cost= 0.079299077 W= 0.27665815 b= 0.6067654\n",
      "Epoch: 0450 cost= 0.079030409 W= 0.27504855 b= 0.618345\n",
      "Epoch: 0500 cost= 0.078792892 W= 0.2735348 b= 0.62923497\n",
      "Epoch: 0550 cost= 0.078582957 W= 0.2721111 b= 0.63947695\n",
      "Epoch: 0600 cost= 0.078397334 W= 0.27077198 b= 0.6491103\n",
      "Epoch: 0650 cost= 0.078233257 W= 0.26951253 b= 0.6581708\n",
      "Epoch: 0700 cost= 0.078088194 W= 0.26832795 b= 0.66669285\n",
      "Epoch: 0750 cost= 0.077959977 W= 0.26721373 b= 0.6747083\n",
      "Epoch: 0800 cost= 0.077846669 W= 0.26616588 b= 0.6822463\n",
      "Epoch: 0850 cost= 0.077746481 W= 0.26518032 b= 0.6893366\n",
      "Epoch: 0900 cost= 0.077657983 W= 0.2642532 b= 0.69600546\n",
      "Epoch: 0950 cost= 0.077579744 W= 0.2633815 b= 0.70227754\n",
      "Epoch: 1000 cost= 0.077510610 W= 0.2625613 b= 0.7081771\n",
      "Optimization Finished!\n",
      "Training cost= 0.07751061 W= 0.2625613 b= 0.7081771 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14bbd390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (x, y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
    "                \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
    "    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n",
    "\n",
    "    #Graphic display\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 Logistic Regression Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.184012805\n",
      "Epoch: 0002 cost= 0.665294415\n",
      "Epoch: 0003 cost= 0.552762057\n",
      "Epoch: 0004 cost= 0.498673494\n",
      "Epoch: 0005 cost= 0.465560877\n",
      "Epoch: 0006 cost= 0.442580879\n",
      "Epoch: 0007 cost= 0.425496204\n",
      "Epoch: 0008 cost= 0.412230986\n",
      "Epoch: 0009 cost= 0.401394205\n",
      "Epoch: 0010 cost= 0.392436447\n",
      "Epoch: 0011 cost= 0.384711898\n",
      "Epoch: 0012 cost= 0.378175347\n",
      "Epoch: 0013 cost= 0.372413592\n",
      "Epoch: 0014 cost= 0.367285245\n",
      "Epoch: 0015 cost= 0.362754928\n",
      "Epoch: 0016 cost= 0.358589045\n",
      "Epoch: 0017 cost= 0.354855685\n",
      "Epoch: 0018 cost= 0.351404875\n",
      "Epoch: 0019 cost= 0.348333276\n",
      "Epoch: 0020 cost= 0.345451373\n",
      "Epoch: 0021 cost= 0.342762793\n",
      "Epoch: 0022 cost= 0.340200319\n",
      "Epoch: 0023 cost= 0.337917796\n",
      "Epoch: 0024 cost= 0.335706564\n",
      "Epoch: 0025 cost= 0.333698747\n",
      "Optimization Finished!\n",
      "Accuracy: 0.889\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Nearest Neighbor Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, we limit mnist data\n",
    "Xtr, Ytr = mnist.train.next_batch(5000) #5000 for training (nn candidates)\n",
    "Xte, Yte = mnist.test.next_batch(200) #200 for testing\n",
    "\n",
    "# tf Graph Input\n",
    "xtr = tf.placeholder(\"float\", [None, 784])\n",
    "xte = tf.placeholder(\"float\", [784])\n",
    "\n",
    "# Nearest Neighbor calculation using L1 Distance\n",
    "# Calculate L1 Distance\n",
    "distance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.negative(xte))), reduction_indices=1)\n",
    "# Prediction: Get min distance index (Nearest neighbor)\n",
    "pred = tf.argmin(distance, 0)\n",
    "\n",
    "accuracy = 0.\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0 Prediction: 7 True Class: 7\n",
      "Test 1 Prediction: 7 True Class: 7\n",
      "Test 2 Prediction: 2 True Class: 2\n",
      "Test 3 Prediction: 7 True Class: 7\n",
      "Test 4 Prediction: 9 True Class: 9\n",
      "Test 5 Prediction: 6 True Class: 6\n",
      "Test 6 Prediction: 9 True Class: 9\n",
      "Test 7 Prediction: 7 True Class: 7\n",
      "Test 8 Prediction: 0 True Class: 0\n",
      "Test 9 Prediction: 2 True Class: 0\n",
      "Test 10 Prediction: 9 True Class: 9\n",
      "Test 11 Prediction: 1 True Class: 1\n",
      "Test 12 Prediction: 2 True Class: 2\n",
      "Test 13 Prediction: 6 True Class: 6\n",
      "Test 14 Prediction: 4 True Class: 4\n",
      "Test 15 Prediction: 6 True Class: 6\n",
      "Test 16 Prediction: 9 True Class: 9\n",
      "Test 17 Prediction: 1 True Class: 1\n",
      "Test 18 Prediction: 7 True Class: 7\n",
      "Test 19 Prediction: 3 True Class: 3\n",
      "Test 20 Prediction: 8 True Class: 8\n",
      "Test 21 Prediction: 6 True Class: 6\n",
      "Test 22 Prediction: 9 True Class: 9\n",
      "Test 23 Prediction: 5 True Class: 5\n",
      "Test 24 Prediction: 6 True Class: 6\n",
      "Test 25 Prediction: 9 True Class: 9\n",
      "Test 26 Prediction: 9 True Class: 9\n",
      "Test 27 Prediction: 2 True Class: 2\n",
      "Test 28 Prediction: 4 True Class: 4\n",
      "Test 29 Prediction: 7 True Class: 7\n",
      "Test 30 Prediction: 1 True Class: 1\n",
      "Test 31 Prediction: 0 True Class: 0\n",
      "Test 32 Prediction: 2 True Class: 2\n",
      "Test 33 Prediction: 4 True Class: 4\n",
      "Test 34 Prediction: 1 True Class: 1\n",
      "Test 35 Prediction: 4 True Class: 4\n",
      "Test 36 Prediction: 6 True Class: 6\n",
      "Test 37 Prediction: 9 True Class: 9\n",
      "Test 38 Prediction: 7 True Class: 7\n",
      "Test 39 Prediction: 3 True Class: 3\n",
      "Test 40 Prediction: 3 True Class: 3\n",
      "Test 41 Prediction: 1 True Class: 8\n",
      "Test 42 Prediction: 7 True Class: 9\n",
      "Test 43 Prediction: 1 True Class: 1\n",
      "Test 44 Prediction: 1 True Class: 1\n",
      "Test 45 Prediction: 8 True Class: 8\n",
      "Test 46 Prediction: 1 True Class: 1\n",
      "Test 47 Prediction: 2 True Class: 2\n",
      "Test 48 Prediction: 1 True Class: 1\n",
      "Test 49 Prediction: 0 True Class: 0\n",
      "Test 50 Prediction: 1 True Class: 1\n",
      "Test 51 Prediction: 9 True Class: 9\n",
      "Test 52 Prediction: 4 True Class: 4\n",
      "Test 53 Prediction: 4 True Class: 4\n",
      "Test 54 Prediction: 4 True Class: 4\n",
      "Test 55 Prediction: 9 True Class: 9\n",
      "Test 56 Prediction: 5 True Class: 5\n",
      "Test 57 Prediction: 1 True Class: 1\n",
      "Test 58 Prediction: 9 True Class: 9\n",
      "Test 59 Prediction: 8 True Class: 8\n",
      "Test 60 Prediction: 2 True Class: 2\n",
      "Test 61 Prediction: 2 True Class: 2\n",
      "Test 62 Prediction: 1 True Class: 1\n",
      "Test 63 Prediction: 1 True Class: 1\n",
      "Test 64 Prediction: 7 True Class: 7\n",
      "Test 65 Prediction: 2 True Class: 2\n",
      "Test 66 Prediction: 0 True Class: 0\n",
      "Test 67 Prediction: 2 True Class: 2\n",
      "Test 68 Prediction: 8 True Class: 8\n",
      "Test 69 Prediction: 8 True Class: 8\n",
      "Test 70 Prediction: 4 True Class: 4\n",
      "Test 71 Prediction: 4 True Class: 4\n",
      "Test 72 Prediction: 4 True Class: 4\n",
      "Test 73 Prediction: 0 True Class: 0\n",
      "Test 74 Prediction: 8 True Class: 8\n",
      "Test 75 Prediction: 4 True Class: 4\n",
      "Test 76 Prediction: 3 True Class: 3\n",
      "Test 77 Prediction: 1 True Class: 1\n",
      "Test 78 Prediction: 8 True Class: 8\n",
      "Test 79 Prediction: 0 True Class: 0\n",
      "Test 80 Prediction: 1 True Class: 7\n",
      "Test 81 Prediction: 9 True Class: 9\n",
      "Test 82 Prediction: 0 True Class: 3\n",
      "Test 83 Prediction: 3 True Class: 3\n",
      "Test 84 Prediction: 3 True Class: 3\n",
      "Test 85 Prediction: 0 True Class: 0\n",
      "Test 86 Prediction: 6 True Class: 6\n",
      "Test 87 Prediction: 4 True Class: 4\n",
      "Test 88 Prediction: 3 True Class: 3\n",
      "Test 89 Prediction: 1 True Class: 1\n",
      "Test 90 Prediction: 8 True Class: 8\n",
      "Test 91 Prediction: 7 True Class: 7\n",
      "Test 92 Prediction: 7 True Class: 9\n",
      "Test 93 Prediction: 9 True Class: 9\n",
      "Test 94 Prediction: 1 True Class: 1\n",
      "Test 95 Prediction: 7 True Class: 5\n",
      "Test 96 Prediction: 0 True Class: 0\n",
      "Test 97 Prediction: 5 True Class: 5\n",
      "Test 98 Prediction: 2 True Class: 2\n",
      "Test 99 Prediction: 7 True Class: 7\n",
      "Test 100 Prediction: 3 True Class: 3\n",
      "Test 101 Prediction: 0 True Class: 0\n",
      "Test 102 Prediction: 7 True Class: 7\n",
      "Test 103 Prediction: 9 True Class: 9\n",
      "Test 104 Prediction: 0 True Class: 0\n",
      "Test 105 Prediction: 9 True Class: 9\n",
      "Test 106 Prediction: 5 True Class: 5\n",
      "Test 107 Prediction: 2 True Class: 2\n",
      "Test 108 Prediction: 7 True Class: 7\n",
      "Test 109 Prediction: 5 True Class: 5\n",
      "Test 110 Prediction: 4 True Class: 4\n",
      "Test 111 Prediction: 1 True Class: 1\n",
      "Test 112 Prediction: 8 True Class: 8\n",
      "Test 113 Prediction: 4 True Class: 4\n",
      "Test 114 Prediction: 7 True Class: 7\n",
      "Test 115 Prediction: 4 True Class: 4\n",
      "Test 116 Prediction: 0 True Class: 2\n",
      "Test 117 Prediction: 2 True Class: 2\n",
      "Test 118 Prediction: 8 True Class: 8\n",
      "Test 119 Prediction: 3 True Class: 3\n",
      "Test 120 Prediction: 5 True Class: 5\n",
      "Test 121 Prediction: 9 True Class: 9\n",
      "Test 122 Prediction: 2 True Class: 2\n",
      "Test 123 Prediction: 5 True Class: 5\n",
      "Test 124 Prediction: 2 True Class: 2\n",
      "Test 125 Prediction: 4 True Class: 4\n",
      "Test 126 Prediction: 2 True Class: 2\n",
      "Test 127 Prediction: 1 True Class: 1\n",
      "Test 128 Prediction: 0 True Class: 0\n",
      "Test 129 Prediction: 1 True Class: 3\n",
      "Test 130 Prediction: 7 True Class: 7\n",
      "Test 131 Prediction: 8 True Class: 8\n",
      "Test 132 Prediction: 6 True Class: 6\n",
      "Test 133 Prediction: 9 True Class: 9\n",
      "Test 134 Prediction: 4 True Class: 4\n",
      "Test 135 Prediction: 0 True Class: 0\n",
      "Test 136 Prediction: 6 True Class: 6\n",
      "Test 137 Prediction: 7 True Class: 7\n",
      "Test 138 Prediction: 4 True Class: 4\n",
      "Test 139 Prediction: 3 True Class: 3\n",
      "Test 140 Prediction: 1 True Class: 1\n",
      "Test 141 Prediction: 2 True Class: 2\n",
      "Test 142 Prediction: 6 True Class: 6\n",
      "Test 143 Prediction: 5 True Class: 5\n",
      "Test 144 Prediction: 9 True Class: 9\n",
      "Test 145 Prediction: 2 True Class: 2\n",
      "Test 146 Prediction: 6 True Class: 6\n",
      "Test 147 Prediction: 4 True Class: 4\n",
      "Test 148 Prediction: 2 True Class: 2\n",
      "Test 149 Prediction: 7 True Class: 7\n",
      "Test 150 Prediction: 3 True Class: 3\n",
      "Test 151 Prediction: 9 True Class: 7\n",
      "Test 152 Prediction: 7 True Class: 7\n",
      "Test 153 Prediction: 0 True Class: 2\n",
      "Test 154 Prediction: 4 True Class: 4\n",
      "Test 155 Prediction: 7 True Class: 7\n",
      "Test 156 Prediction: 8 True Class: 8\n",
      "Test 157 Prediction: 2 True Class: 2\n",
      "Test 158 Prediction: 8 True Class: 8\n",
      "Test 159 Prediction: 7 True Class: 7\n",
      "Test 160 Prediction: 0 True Class: 0\n",
      "Test 161 Prediction: 1 True Class: 1\n",
      "Test 162 Prediction: 5 True Class: 5\n",
      "Test 163 Prediction: 4 True Class: 6\n",
      "Test 164 Prediction: 6 True Class: 6\n",
      "Test 165 Prediction: 1 True Class: 1\n",
      "Test 166 Prediction: 9 True Class: 9\n",
      "Test 167 Prediction: 9 True Class: 9\n",
      "Test 168 Prediction: 3 True Class: 3\n",
      "Test 169 Prediction: 3 True Class: 3\n",
      "Test 170 Prediction: 2 True Class: 2\n",
      "Test 171 Prediction: 7 True Class: 7\n",
      "Test 172 Prediction: 9 True Class: 9\n",
      "Test 173 Prediction: 9 True Class: 9\n",
      "Test 174 Prediction: 8 True Class: 8\n",
      "Test 175 Prediction: 2 True Class: 2\n",
      "Test 176 Prediction: 4 True Class: 4\n",
      "Test 177 Prediction: 4 True Class: 4\n",
      "Test 178 Prediction: 2 True Class: 2\n",
      "Test 179 Prediction: 1 True Class: 1\n",
      "Test 180 Prediction: 0 True Class: 0\n",
      "Test 181 Prediction: 7 True Class: 7\n",
      "Test 182 Prediction: 7 True Class: 7\n",
      "Test 183 Prediction: 1 True Class: 1\n",
      "Test 184 Prediction: 2 True Class: 2\n",
      "Test 185 Prediction: 6 True Class: 6\n",
      "Test 186 Prediction: 0 True Class: 0\n",
      "Test 187 Prediction: 8 True Class: 8\n",
      "Test 188 Prediction: 5 True Class: 5\n",
      "Test 189 Prediction: 1 True Class: 1\n",
      "Test 190 Prediction: 9 True Class: 9\n",
      "Test 191 Prediction: 9 True Class: 9\n",
      "Test 192 Prediction: 8 True Class: 8\n",
      "Test 193 Prediction: 3 True Class: 3\n",
      "Test 194 Prediction: 1 True Class: 1\n",
      "Test 195 Prediction: 7 True Class: 7\n",
      "Test 196 Prediction: 2 True Class: 2\n",
      "Test 197 Prediction: 6 True Class: 6\n",
      "Test 198 Prediction: 6 True Class: 6\n",
      "Test 199 Prediction: 1 True Class: 1\n",
      "Done!\n",
      "Accuracy: 0.9400000000000007\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # loop over test data\n",
    "    for i in range(len(Xte)):\n",
    "        # Get nearest neighbor\n",
    "        nn_index = sess.run(pred, feed_dict={xtr: Xtr, xte: Xte[i, :]})\n",
    "        # Get nearest neighbor class label and compare it to its true label\n",
    "        print(\"Test\", i, \"Prediction:\", np.argmax(Ytr[nn_index]), \\\n",
    "            \"True Class:\", np.argmax(Yte[i]))\n",
    "        # Calculate accuracy\n",
    "        if np.argmax(Ytr[nn_index]) == np.argmax(Yte[i]):\n",
    "            accuracy += 1./len(Xte)\n",
    "    print(\"Done!\")\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.factorization import KMeans\n",
    "\n",
    "# Ignore all GPUs, tf random forest does not benefit from it.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "full_data_x = mnist.train.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_steps = 50 # Total steps to train\n",
    "batch_size = 1024 # The number of samples per batch\n",
    "k = 25 # The number of clusters\n",
    "num_classes = 10 # The 10 digits\n",
    "num_features = 784 # Each image is 28x28 pixels\n",
    "\n",
    "# Input images\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "# Labels (for assigning a label to a centroid and testing)\n",
    "Y = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
    "\n",
    "# K-Means Parameters\n",
    "kmeans = KMeans(inputs=X, num_clusters=k, distance_metric='cosine',\n",
    "                use_mini_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\win7\\python36\\lib\\site-packages\\tensorflow\\contrib\\factorization\\python\\ops\\clustering_ops.py:743: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-aafa883c8a4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Build KMeans graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m (all_scores, cluster_idx, scores, cluster_centers_initialized, \n\u001b[1;32m----> 3\u001b[1;33m  cluster_centers_vars,init_op,train_op) = kmeans.training_graph()\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mcluster_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# fix for cluster_idx being a tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mavg_distance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 6)"
     ]
    }
   ],
   "source": [
    "# Build KMeans graph\n",
    "(all_scores, cluster_idx, scores, cluster_centers_initialized, \n",
    " cluster_centers_vars,init_op,train_op) = kmeans.training_graph()\n",
    "cluster_idx = cluster_idx[0] # fix for cluster_idx being a tuple\n",
    "avg_distance = tf.reduce_mean(scores)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init_vars = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorFlow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init_vars, feed_dict={X: full_data_x})\n",
    "sess.run(init_op, feed_dict={X: full_data_x})\n",
    "\n",
    "# Training\n",
    "for i in range(1, num_steps + 1):\n",
    "    _, d, idx = sess.run([train_op, avg_distance, cluster_idx],\n",
    "                         feed_dict={X: full_data_x})\n",
    "    if i % 10 == 0 or i == 1:\n",
    "        print(\"Step %i, Avg Distance: %f\" % (i, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a label to each centroid\n",
    "# Count total number of labels per centroid, using the label of each training\n",
    "# sample to their closest centroid (given by 'idx')\n",
    "counts = np.zeros(shape=(k, num_classes))\n",
    "for i in range(len(idx)):\n",
    "    counts[idx[i]] += mnist.train.labels[i]\n",
    "# Assign the most frequent label to the centroid\n",
    "labels_map = [np.argmax(c) for c in counts]\n",
    "labels_map = tf.convert_to_tensor(labels_map)\n",
    "\n",
    "# Evaluation ops\n",
    "# Lookup: centroid_id -> label\n",
    "cluster_label = tf.nn.embedding_lookup(labels_map, cluster_idx)\n",
    "# Compute accuracy\n",
    "correct_prediction = tf.equal(cluster_label, tf.cast(tf.argmax(Y, 1), tf.int32))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Test Model\n",
    "test_x, test_y = mnist.test.images, mnist.test.labels\n",
    "print(\"Test Accuracy:\", sess.run(accuracy_op, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 Random Forest Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import resources\n",
    "from tensorflow.contrib.tensor_forest.python import tensor_forest\n",
    "\n",
    "# Ignore all GPUs, tf random forest does not benefit from it.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_steps = 500 # Total steps to train\n",
    "batch_size = 1024 # The number of samples per batch\n",
    "num_classes = 10 # The 10 digits\n",
    "num_features = 784 # Each image is 28x28 pixels\n",
    "num_trees = 10\n",
    "max_nodes = 1000\n",
    "\n",
    "# Input and Target data\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "# For random forest, labels must be integers (the class id)\n",
    "Y = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "# Random Forest Parameters\n",
    "hparams = tensor_forest.ForestHParams(num_classes=num_classes,\n",
    "                                      num_features=num_features,\n",
    "                                      num_trees=num_trees,\n",
    "                                      max_nodes=max_nodes).fill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Random Forest\n",
    "forest_graph = tensor_forest.RandomForestGraphs(hparams)\n",
    "# Get training graph and loss\n",
    "train_op = forest_graph.training_graph(X, Y)\n",
    "loss_op = forest_graph.training_loss(X, Y)\n",
    "\n",
    "# Measure the accuracy\n",
    "infer_op, _, _ = forest_graph.inference_graph(X)\n",
    "correct_prediction = tf.equal(tf.argmax(infer_op, 1), tf.cast(Y, tf.int64))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value) and forest resources\n",
    "init_vars = tf.group(tf.global_variables_initializer(),\n",
    "    resources.initialize_resources(resources.shared_resources()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorFlow session\n",
    "sess = tf.train.MonitoredSession()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init_vars)\n",
    "\n",
    "# Training\n",
    "for i in range(1, num_steps + 1):\n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    _, l = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})\n",
    "    if i % 50 == 0 or i == 1:\n",
    "        acc = sess.run(accuracy_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        print('Step %i, Loss: %f, Acc: %f' % (i, l, acc))\n",
    "\n",
    "# Test Model\n",
    "test_x, test_y = mnist.test.images, mnist.test.labels\n",
    "print(\"Test Accuracy:\", sess.run(accuracy_op, feed_dict={X: test_x, Y: test_y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 Neural Network Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.1\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                      Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 Convolutional Neural Network Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-36-3814cdbf73dc>:26: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 66688.1094, Training Accuracy= 0.125\n",
      "Step 10, Minibatch Loss= 24767.6562, Training Accuracy= 0.180\n",
      "Step 20, Minibatch Loss= 8763.7109, Training Accuracy= 0.578\n",
      "Step 30, Minibatch Loss= 4404.4004, Training Accuracy= 0.758\n",
      "Step 40, Minibatch Loss= 5270.2671, Training Accuracy= 0.773\n",
      "Step 50, Minibatch Loss= 5499.8945, Training Accuracy= 0.766\n",
      "Step 60, Minibatch Loss= 3609.9519, Training Accuracy= 0.797\n",
      "Step 70, Minibatch Loss= 3118.3298, Training Accuracy= 0.852\n",
      "Step 80, Minibatch Loss= 2590.6042, Training Accuracy= 0.859\n",
      "Step 90, Minibatch Loss= 2904.8364, Training Accuracy= 0.867\n",
      "Step 100, Minibatch Loss= 4169.4771, Training Accuracy= 0.805\n",
      "Step 110, Minibatch Loss= 1428.4148, Training Accuracy= 0.922\n",
      "Step 120, Minibatch Loss= 1834.9479, Training Accuracy= 0.922\n",
      "Step 130, Minibatch Loss= 2156.4587, Training Accuracy= 0.875\n",
      "Step 140, Minibatch Loss= 1535.2532, Training Accuracy= 0.938\n",
      "Step 150, Minibatch Loss= 2586.8708, Training Accuracy= 0.836\n",
      "Step 160, Minibatch Loss= 1630.9736, Training Accuracy= 0.859\n",
      "Step 170, Minibatch Loss= 1950.6243, Training Accuracy= 0.898\n",
      "Step 180, Minibatch Loss= 810.0112, Training Accuracy= 0.945\n",
      "Step 190, Minibatch Loss= 1275.4979, Training Accuracy= 0.906\n",
      "Step 200, Minibatch Loss= 450.2177, Training Accuracy= 0.961\n",
      "Step 210, Minibatch Loss= 1912.4471, Training Accuracy= 0.914\n",
      "Step 220, Minibatch Loss= 263.0989, Training Accuracy= 0.961\n",
      "Step 230, Minibatch Loss= 866.9370, Training Accuracy= 0.953\n",
      "Step 240, Minibatch Loss= 656.2336, Training Accuracy= 0.938\n",
      "Step 250, Minibatch Loss= 1587.0867, Training Accuracy= 0.922\n",
      "Step 260, Minibatch Loss= 1327.6675, Training Accuracy= 0.945\n",
      "Step 270, Minibatch Loss= 992.2573, Training Accuracy= 0.930\n",
      "Step 280, Minibatch Loss= 1074.2915, Training Accuracy= 0.938\n",
      "Step 290, Minibatch Loss= 1543.5764, Training Accuracy= 0.898\n",
      "Step 300, Minibatch Loss= 1375.9258, Training Accuracy= 0.922\n",
      "Step 310, Minibatch Loss= 853.0047, Training Accuracy= 0.914\n",
      "Step 320, Minibatch Loss= 874.2735, Training Accuracy= 0.945\n",
      "Step 330, Minibatch Loss= 1868.6622, Training Accuracy= 0.930\n",
      "Step 340, Minibatch Loss= 701.5262, Training Accuracy= 0.945\n",
      "Step 350, Minibatch Loss= 1199.7882, Training Accuracy= 0.914\n",
      "Step 360, Minibatch Loss= 1635.0303, Training Accuracy= 0.914\n",
      "Step 370, Minibatch Loss= 448.3743, Training Accuracy= 0.945\n",
      "Step 380, Minibatch Loss= 655.2205, Training Accuracy= 0.938\n",
      "Step 390, Minibatch Loss= 1290.9709, Training Accuracy= 0.914\n",
      "Step 400, Minibatch Loss= 813.4697, Training Accuracy= 0.938\n",
      "Step 410, Minibatch Loss= 1118.9133, Training Accuracy= 0.930\n",
      "Step 420, Minibatch Loss= 818.8732, Training Accuracy= 0.922\n",
      "Step 430, Minibatch Loss= 385.9644, Training Accuracy= 0.969\n",
      "Step 440, Minibatch Loss= 520.3712, Training Accuracy= 0.945\n",
      "Step 450, Minibatch Loss= 829.9753, Training Accuracy= 0.945\n",
      "Step 460, Minibatch Loss= 763.8250, Training Accuracy= 0.969\n",
      "Step 470, Minibatch Loss= 1060.9261, Training Accuracy= 0.938\n",
      "Step 480, Minibatch Loss= 404.9393, Training Accuracy= 0.969\n",
      "Step 490, Minibatch Loss= 81.6794, Training Accuracy= 0.977\n",
      "Step 500, Minibatch Loss= 649.4042, Training Accuracy= 0.945\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.97265625\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
    "                                      Y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Recurrent Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_hidden' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-10c9755fe36c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Define weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m weights = {\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;34m'out'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m }\n\u001b[0;32m      5\u001b[0m biases = {\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_hidden' is not defined"
     ]
    }
   ],
   "source": [
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = RNN(X, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto-Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 30000\n",
    "batch_size = 256\n",
    "\n",
    "display_step = 1000\n",
    "examples_to_show = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 256 # 1st layer num features\n",
    "num_hidden_2 = 128 # 2nd layer num features (the latent dim)\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Minibatch Loss: 0.426531\n",
      "Step 1000: Minibatch Loss: 0.149390\n",
      "Step 2000: Minibatch Loss: 0.132865\n",
      "Step 3000: Minibatch Loss: 0.118904\n",
      "Step 4000: Minibatch Loss: 0.111283\n",
      "Step 5000: Minibatch Loss: 0.109543\n",
      "Step 6000: Minibatch Loss: 0.107499\n",
      "Step 7000: Minibatch Loss: 0.102314\n",
      "Step 8000: Minibatch Loss: 0.097982\n",
      "Step 9000: Minibatch Loss: 0.095127\n",
      "Step 10000: Minibatch Loss: 0.091095\n",
      "Step 11000: Minibatch Loss: 0.088785\n",
      "Step 12000: Minibatch Loss: 0.086447\n",
      "Step 13000: Minibatch Loss: 0.085255\n",
      "Step 14000: Minibatch Loss: 0.084466\n",
      "Step 15000: Minibatch Loss: 0.080597\n",
      "Step 16000: Minibatch Loss: 0.083219\n",
      "Step 17000: Minibatch Loss: 0.079773\n",
      "Step 18000: Minibatch Loss: 0.072739\n",
      "Step 19000: Minibatch Loss: 0.073016\n",
      "Step 20000: Minibatch Loss: 0.070689\n",
      "Step 21000: Minibatch Loss: 0.068201\n",
      "Step 22000: Minibatch Loss: 0.067601\n",
      "Step 23000: Minibatch Loss: 0.066161\n",
      "Step 24000: Minibatch Loss: 0.063168\n",
      "Step 25000: Minibatch Loss: 0.061067\n",
      "Step 26000: Minibatch Loss: 0.059777\n",
      "Step 27000: Minibatch Loss: 0.058749\n",
      "Step 28000: Minibatch Loss: 0.058656\n",
      "Step 29000: Minibatch Loss: 0.058717\n",
      "Step 30000: Minibatch Loss: 0.056454\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "# Start a new TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "\n",
    "# Training\n",
    "for i in range(1, num_steps+1):\n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "    _, l = sess.run([optimizer, loss], feed_dict={X: batch_x})\n",
    "    # Display logs per step\n",
    "    if i % display_step == 0 or i == 1:\n",
    "        print('Step %i: Minibatch Loss: %f' % (i, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# Encode and decode images from test set and visualize their reconstruction.\n",
    "n = 4\n",
    "canvas_orig = np.empty((28 * n, 28 * n))\n",
    "canvas_recon = np.empty((28 * n, 28 * n))\n",
    "for i in range(n):\n",
    "    # MNIST test set\n",
    "    batch_x, _ = mnist.test.next_batch(n)\n",
    "    # Encode and decode the digit image\n",
    "    g = sess.run(decoder_op, feed_dict={X: batch_x})\n",
    "    \n",
    "    # Display original images\n",
    "    for j in range(n):\n",
    "        # Draw the generated digits\n",
    "        canvas_orig[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = batch_x[j].reshape([28, 28])\n",
    "    # Display reconstructed images\n",
    "    for j in range(n):\n",
    "        # Draw the generated digits\n",
    "        canvas_recon[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = g[j].reshape([28, 28])\n",
    "\n",
    "print(\"Original Images\")     \n",
    "plt.figure(figsize=(n, n))\n",
    "plt.imshow(canvas_orig, origin=\"upper\", cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Reconstructed Images\")\n",
    "plt.figure(figsize=(n, n))\n",
    "plt.imshow(canvas_recon, origin=\"upper\", cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 30000\n",
    "batch_size = 64\n",
    "\n",
    "# Network Parameters\n",
    "image_dim = 784 # MNIST images are 28x28 pixels\n",
    "hidden_dim = 512\n",
    "latent_dim = 2\n",
    "\n",
    "# A custom initialization (see Xavier Glorot init)\n",
    "def glorot_init(shape):\n",
    "    return tf.random_normal(shape=shape, stddev=1. / tf.sqrt(shape[0] / 2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(glorot_init([image_dim, hidden_dim])),\n",
    "    'z_mean': tf.Variable(glorot_init([hidden_dim, latent_dim])),\n",
    "    'z_std': tf.Variable(glorot_init([hidden_dim, latent_dim])),\n",
    "    'decoder_h1': tf.Variable(glorot_init([latent_dim, hidden_dim])),\n",
    "    'decoder_out': tf.Variable(glorot_init([hidden_dim, image_dim]))\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(glorot_init([hidden_dim])),\n",
    "    'z_mean': tf.Variable(glorot_init([latent_dim])),\n",
    "    'z_std': tf.Variable(glorot_init([latent_dim])),\n",
    "    'decoder_b1': tf.Variable(glorot_init([hidden_dim])),\n",
    "    'decoder_out': tf.Variable(glorot_init([image_dim]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "input_image = tf.placeholder(tf.float32, shape=[None, image_dim])\n",
    "encoder = tf.matmul(input_image, weights['encoder_h1']) + biases['encoder_b1']\n",
    "encoder = tf.nn.tanh(encoder)\n",
    "z_mean = tf.matmul(encoder, weights['z_mean']) + biases['z_mean']\n",
    "z_std = tf.matmul(encoder, weights['z_std']) + biases['z_std']\n",
    "\n",
    "# Sampler: Normal (gaussian) random distribution\n",
    "eps = tf.random_normal(tf.shape(z_std), dtype=tf.float32, mean=0., stddev=1.0,\n",
    "                       name='epsilon')\n",
    "z = z_mean + tf.exp(z_std / 2) * eps\n",
    "\n",
    "# Building the decoder (with scope to re-use these layers later)\n",
    "decoder = tf.matmul(z, weights['decoder_h1']) + biases['decoder_b1']\n",
    "decoder = tf.nn.tanh(decoder)\n",
    "decoder = tf.matmul(decoder, weights['decoder_out']) + biases['decoder_out']\n",
    "decoder = tf.nn.sigmoid(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE Loss\n",
    "def vae_loss(x_reconstructed, x_true):\n",
    "    # Reconstruction loss\n",
    "    encode_decode_loss = x_true * tf.log(1e-10 + x_reconstructed) \\\n",
    "                         + (1 - x_true) * tf.log(1e-10 + 1 - x_reconstructed)\n",
    "    encode_decode_loss = -tf.reduce_sum(encode_decode_loss, 1)\n",
    "    # KL Divergence loss\n",
    "    kl_div_loss = 1 + z_std - tf.square(z_mean) - tf.exp(z_std)\n",
    "    kl_div_loss = -0.5 * tf.reduce_sum(kl_div_loss, 1)\n",
    "    return tf.reduce_mean(encode_decode_loss + kl_div_loss)\n",
    "\n",
    "loss_op = vae_loss(decoder, input_image)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "# Start a new TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "\n",
    "# Training\n",
    "for i in range(1, num_steps+1):\n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "    # Train\n",
    "    feed_dict = {input_image: batch_x}\n",
    "    _, l = sess.run([train_op, loss_op], feed_dict=feed_dict)\n",
    "    if i % 1000 == 0 or i == 1:\n",
    "        print('Step %i, Loss: %f' % (i, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# Generator takes noise as input\n",
    "noise_input = tf.placeholder(tf.float32, shape=[None, latent_dim])\n",
    "# Rebuild the decoder to create image from noise\n",
    "decoder = tf.matmul(noise_input, weights['decoder_h1']) + biases['decoder_b1']\n",
    "decoder = tf.nn.tanh(decoder)\n",
    "decoder = tf.matmul(decoder, weights['decoder_out']) + biases['decoder_out']\n",
    "decoder = tf.nn.sigmoid(decoder)\n",
    "\n",
    "# Building a manifold of generated digits\n",
    "n = 20\n",
    "x_axis = np.linspace(-3, 3, n)\n",
    "y_axis = np.linspace(-3, 3, n)\n",
    "\n",
    "canvas = np.empty((28 * n, 28 * n))\n",
    "for i, yi in enumerate(x_axis):\n",
    "    for j, xi in enumerate(y_axis):\n",
    "        z_mu = np.array([[xi, yi]] * batch_size)\n",
    "        x_mean = sess.run(decoder, feed_dict={noise_input: z_mu})\n",
    "        canvas[(n - i - 1) * 28:(n - i) * 28, j * 28:(j + 1) * 28] = \\\n",
    "        x_mean[0].reshape(28, 28)\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "Xi, Yi = np.meshgrid(x_axis, y_axis)\n",
    "plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "num_steps = 70000\n",
    "batch_size = 128\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Network Params\n",
    "image_dim = 784 # 28*28 pixels\n",
    "gen_hidden_dim = 256\n",
    "disc_hidden_dim = 256\n",
    "noise_dim = 100 # Noise data points\n",
    "\n",
    "# A custom initialization (see Xavier Glorot init)\n",
    "def glorot_init(shape):\n",
    "    return tf.random_normal(shape=shape, stddev=1. / tf.sqrt(shape[0] / 2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'gen_hidden1': tf.Variable(glorot_init([noise_dim, gen_hidden_dim])),\n",
    "    'gen_out': tf.Variable(glorot_init([gen_hidden_dim, image_dim])),\n",
    "    'disc_hidden1': tf.Variable(glorot_init([image_dim, disc_hidden_dim])),\n",
    "    'disc_out': tf.Variable(glorot_init([disc_hidden_dim, 1])),\n",
    "}\n",
    "biases = {\n",
    "    'gen_hidden1': tf.Variable(tf.zeros([gen_hidden_dim])),\n",
    "    'gen_out': tf.Variable(tf.zeros([image_dim])),\n",
    "    'disc_hidden1': tf.Variable(tf.zeros([disc_hidden_dim])),\n",
    "    'disc_out': tf.Variable(tf.zeros([1])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "def generator(x):\n",
    "    hidden_layer = tf.matmul(x, weights['gen_hidden1'])\n",
    "    hidden_layer = tf.add(hidden_layer, biases['gen_hidden1'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    out_layer = tf.matmul(hidden_layer, weights['gen_out'])\n",
    "    out_layer = tf.add(out_layer, biases['gen_out'])\n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "def discriminator(x):\n",
    "    hidden_layer = tf.matmul(x, weights['disc_hidden1'])\n",
    "    hidden_layer = tf.add(hidden_layer, biases['disc_hidden1'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    out_layer = tf.matmul(hidden_layer, weights['disc_out'])\n",
    "    out_layer = tf.add(out_layer, biases['disc_out'])\n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    return out_layer\n",
    "\n",
    "# Build Networks\n",
    "# Network Inputs\n",
    "gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name='input_noise')\n",
    "disc_input = tf.placeholder(tf.float32, shape=[None, image_dim], name='disc_input')\n",
    "\n",
    "# Build Generator Network\n",
    "gen_sample = generator(gen_input)\n",
    "\n",
    "# Build 2 Discriminator Networks (one from noise input, one from generated samples)\n",
    "disc_real = discriminator(disc_input)\n",
    "disc_fake = discriminator(gen_sample)\n",
    "\n",
    "# Build Loss\n",
    "gen_loss = -tf.reduce_mean(tf.log(disc_fake))\n",
    "disc_loss = -tf.reduce_mean(tf.log(disc_real) + tf.log(1. - disc_fake))\n",
    "\n",
    "# Build Optimizers\n",
    "optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Training Variables for each optimizer\n",
    "# By default in TensorFlow, all variables are updated by each optimizer, so we\n",
    "# need to precise for each one of them the specific variables to update.\n",
    "# Generator Network Variables\n",
    "gen_vars = [weights['gen_hidden1'], weights['gen_out'],\n",
    "            biases['gen_hidden1'], biases['gen_out']]\n",
    "# Discriminator Network Variables\n",
    "disc_vars = [weights['disc_hidden1'], weights['disc_out'],\n",
    "            biases['disc_hidden1'], biases['disc_out']]\n",
    "\n",
    "# Create training operations\n",
    "train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "# Start a new TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "\n",
    "# Training\n",
    "for i in range(1, num_steps+1):\n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "    # Generate noise to feed to the generator\n",
    "    z = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\n",
    "\n",
    "    # Train\n",
    "    feed_dict = {disc_input: batch_x, gen_input: z}\n",
    "    _, _, gl, dl = sess.run([train_gen, train_disc, gen_loss, disc_loss],\n",
    "                            feed_dict=feed_dict)\n",
    "    if i % 2000 == 0 or i == 1:\n",
    "        print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# Generate images from noise, using the generator network.\n",
    "n = 6\n",
    "canvas = np.empty((28 * n, 28 * n))\n",
    "for i in range(n):\n",
    "    # Noise input.\n",
    "    z = np.random.uniform(-1., 1., size=[n, noise_dim])\n",
    "    # Generate image from noise.\n",
    "    g = sess.run(gen_sample, feed_dict={gen_input: z})\n",
    "    # Reverse colours for better display\n",
    "    g = -1 * (g - 1)\n",
    "    for j in range(n):\n",
    "        # Draw the generated digits\n",
    "        canvas[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = g[j].reshape([28, 28])\n",
    "\n",
    "plt.figure(figsize=(n, n))\n",
    "plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
